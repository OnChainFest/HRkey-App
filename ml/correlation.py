#!/usr/bin/env python3
"""
HRKey - Proof of Correlation MVP (Simplified)
==============================================

Script simplificado para el MVP del Proof of Correlation.
Solo usa dos tablas: user_kpis y job_outcomes.
NO depende de la tabla users.

Flujo:
1. Cargar datos de user_kpis y job_outcomes
2. Hacer merge por user_id
3. Calcular correlaciones entre KPIs y outcomes
4. Exportar resultados

Autor: HRKey Data Team
Fecha: 2025-11-22 (Simplificado)
"""

import os
import sys
import json
import logging
from datetime import datetime
from typing import Dict, List

import pandas as pd
import numpy as np
from scipy import stats
from dotenv import load_dotenv

# ============================================================================
# CONFIGURACI√ìN
# ============================================================================

# Cargar variables de entorno
load_dotenv()

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# ============================================================================
# CONSTANTES
# ============================================================================

# Tablas de Supabase
TABLE_KPIS = "user_kpis"
TABLE_OUTCOMES = "job_outcomes"

# Columnas de KPIs (ajusta seg√∫n tu schema real)
KPI_COLS = [
    "deployment_frequency",
    "code_quality",
    "api_response_time",
    "error_rate",
    "test_coverage",
    "lead_time",
    "mttr",
    "customer_satisfaction"
]

# Columna de outcome (del schema job_outcomes)
OUTCOME_COL = "performance_score"

# Configuraci√≥n de an√°lisis
MIN_OBSERVATIONS_PER_KPI = 10
OUTPUT_DIR = os.path.join(os.path.dirname(__file__), 'output')

# ============================================================================
# CONEXI√ìN A SUPABASE
# ============================================================================

def get_supabase_connection():
    """
    Obtiene la conexi√≥n a Supabase.

    Returns:
        dict: Configuraci√≥n de conexi√≥n con 'method', 'url', 'key'
    """
    supabase_url = os.getenv('SUPABASE_URL')
    supabase_key = os.getenv('SUPABASE_SERVICE_KEY')

    if not supabase_url or not supabase_key:
        raise ValueError(
            "SUPABASE_URL y SUPABASE_SERVICE_KEY deben estar definidos en .env\n"
            "Ejemplo:\n"
            "  SUPABASE_URL=https://xxx.supabase.co\n"
            "  SUPABASE_SERVICE_KEY=eyJhbGc..."
        )

    # Intentar importar supabase-py
    try:
        from supabase import create_client, Client
        client = create_client(supabase_url, supabase_key)
        logger.info("‚úÖ Conectado a Supabase usando supabase-py")
        return {'method': 'supabase-py', 'client': client}
    except ImportError:
        logger.info("‚ö†Ô∏è  supabase-py no disponible, usando requests directo")
        return {
            'method': 'requests',
            'url': supabase_url,
            'key': supabase_key
        }


def fetch_table(table_name: str) -> pd.DataFrame:
    """
    Carga una tabla completa desde Supabase.

    Args:
        table_name: Nombre de la tabla (user_kpis o job_outcomes)

    Returns:
        pd.DataFrame: DataFrame con todos los datos de la tabla
    """
    logger.info(f"üì• Cargando tabla: {table_name}")

    conn_config = get_supabase_connection()

    # Opci√≥n 1: Usar supabase-py
    if conn_config['method'] == 'supabase-py':
        client = conn_config['client']
        try:
            response = client.table(table_name).select('*').execute()
            data = response.data

            if not data:
                logger.warning(f"‚ö†Ô∏è  Tabla {table_name} est√° vac√≠a")
                return pd.DataFrame()

            df = pd.DataFrame(data)
            logger.info(f"‚úÖ Cargadas {len(df)} filas de {table_name}")
            return df

        except Exception as e:
            logger.error(f"‚ùå Error al cargar {table_name}: {e}")
            raise

    # Opci√≥n 2: Usar requests directo (fallback)
    else:
        import requests

        url = f"{conn_config['url']}/rest/v1/{table_name}"
        headers = {
            'apikey': conn_config['key'],
            'Authorization': f"Bearer {conn_config['key']}",
            'Content-Type': 'application/json'
        }

        try:
            response = requests.get(url, headers=headers, params={'select': '*'})
            response.raise_for_status()
            data = response.json()

            if not data:
                logger.warning(f"‚ö†Ô∏è  Tabla {table_name} est√° vac√≠a")
                return pd.DataFrame()

            df = pd.DataFrame(data)
            logger.info(f"‚úÖ Cargadas {len(df)} filas de {table_name}")
            return df

        except requests.exceptions.RequestException as e:
            logger.error(f"‚ùå Error al cargar {table_name}: {e}")
            raise


# ============================================================================
# CONSTRUCCI√ìN DEL DATASET
# ============================================================================

def build_dataset() -> pd.DataFrame:
    """
    Crea un DataFrame donde cada fila es un usuario con:
      - KPIs (user_kpis)
      - outcome de desempe√±o (job_outcomes)

    Returns:
        pd.DataFrame: Dataset limpio con user_id, KPIs y outcome
    """
    logger.info("\n" + "=" * 80)
    logger.info("CONSTRUYENDO DATASET")
    logger.info("=" * 80)

    # Cargar tablas
    kpis_raw = fetch_table(TABLE_KPIS)
    outcomes = fetch_table(TABLE_OUTCOMES)

    if kpis_raw.empty or outcomes.empty:
        raise RuntimeError(
            "Alguna de las tablas user_kpis / job_outcomes est√° vac√≠a. "
            f"Verifica que ya tengas datos en {TABLE_KPIS} y {TABLE_OUTCOMES}."
        )

    # Verificar que user_kpis tenga las columnas necesarias
    required_kpi_cols = ['user_id', 'kpi_name']
    for col in required_kpi_cols:
        if col not in kpis_raw.columns:
            raise RuntimeError(f"Falta la columna '{col}' en {TABLE_KPIS}.")

    if 'user_id' not in outcomes.columns:
        raise RuntimeError(f"Falta la columna 'user_id' en {TABLE_OUTCOMES}.")

    # Transformar user_kpis de formato largo a formato ancho
    # La tabla user_kpis tiene: user_id, kpi_name, kpi_value, normalized_value, etc.
    # Necesitamos pivotar para que cada KPI sea una columna
    logger.info(f"\nüîÑ Pivotando {TABLE_KPIS} de formato largo a formato ancho...")

    # Decidir qu√© valor usar: normalized_value si existe, sino kpi_value
    value_col = 'normalized_value' if 'normalized_value' in kpis_raw.columns else 'kpi_value'
    if value_col not in kpis_raw.columns:
        raise RuntimeError(
            f"La tabla {TABLE_KPIS} debe tener al menos 'kpi_value' o 'normalized_value'. "
            f"Columnas encontradas: {list(kpis_raw.columns)}"
        )

    logger.info(f"   Usando columna '{value_col}' para valores de KPIs")

    # Pivotar: crear una columna por cada kpi_name
    kpis = kpis_raw.pivot_table(
        index='user_id',
        columns='kpi_name',
        values=value_col,
        aggfunc='mean'  # Si hay m√∫ltiples valores, promediar
    ).reset_index()

    logger.info(f"   KPIs √∫nicos encontrados: {list(kpis.columns[1:])}")
    logger.info(f"   Total usuarios con KPIs: {len(kpis)}")

    # Merge b√°sico solo entre KPIs y outcomes
    logger.info(f"\nüîó Haciendo merge de {TABLE_KPIS} y {TABLE_OUTCOMES} por user_id...")
    df = kpis.merge(outcomes, on="user_id", suffixes=("_kpi", "_outcome"))
    logger.info(f"   Filas despu√©s del merge: {len(df)}")

    # Verificar que exista la columna de outcome
    if OUTCOME_COL not in df.columns:
        raise RuntimeError(
            f"No se encontr√≥ la columna de outcome '{OUTCOME_COL}' en la tabla {TABLE_OUTCOMES}."
        )

    # Identificar columnas de KPIs (todas excepto user_id, outcome, y columnas del merge)
    # Despu√©s del pivot, las columnas de KPIs son las que vinieron de kpi_name
    kpi_cols_in_df = [col for col in df.columns
                      if col not in ['user_id', OUTCOME_COL]
                      and not col.endswith('_kpi')
                      and not col.endswith('_outcome')]

    logger.info(f"\nüìä KPIs encontrados en el dataset: {kpi_cols_in_df}")

    # Filtrar solo columnas relevantes (user_id + KPIs + outcome)
    cols_to_keep = ["user_id"] + kpi_cols_in_df + [OUTCOME_COL]
    df = df[cols_to_keep].copy()

    # Limpieza b√°sica
    logger.info("\nüßπ Limpiando datos...")

    # Quitar filas sin outcome
    original_count = len(df)
    df = df.dropna(subset=[OUTCOME_COL])
    removed_outcome = original_count - len(df)
    if removed_outcome > 0:
        logger.info(f"   Removidas {removed_outcome} filas sin outcome")

    # Quitar filas donde TODOS los KPIs son NULL
    original_count = len(df)
    df = df.dropna(subset=kpi_cols_in_df, how="all")
    removed_kpis = original_count - len(df)
    if removed_kpis > 0:
        logger.info(f"   Removidas {removed_kpis} filas sin ning√∫n KPI")

    logger.info(f"\nüßÆ Dataset final: {df.shape[0]} filas x {df.shape[1]} columnas.")
    logger.info(f"   Columnas: {list(df.columns)}")
    logger.info(f"   KPIs disponibles: {kpi_cols_in_df}")

    return df


# ============================================================================
# C√ÅLCULO DE CORRELACIONES
# ============================================================================

def compute_correlations(df: pd.DataFrame) -> List[Dict]:
    """
    Calcula correlaciones entre cada KPI y el outcome.

    Args:
        df: DataFrame con columnas de KPIs y OUTCOME_COL

    Returns:
        List[Dict]: Lista de resultados con correlaciones por KPI
    """
    logger.info("\n" + "=" * 80)
    logger.info("CALCULANDO CORRELACIONES")
    logger.info("=" * 80)

    if df.empty:
        logger.error("‚ùå DataFrame vac√≠o, no se pueden calcular correlaciones")
        return []

    # Detectar columnas de KPIs (todas excepto user_id y outcome_col)
    kpi_cols_present = [c for c in df.columns if c not in ['user_id', OUTCOME_COL]]

    logger.info(f"\nüìä Analizando {len(kpi_cols_present)} KPIs...")
    logger.info(f"   KPIs: {kpi_cols_present}\n")

    results = []

    for kpi_name in kpi_cols_present:
        logger.info(f"{'‚îÄ' * 80}")
        logger.info(f"KPI: {kpi_name}")

        # Extraer datos v√°lidos (sin NaN)
        valid_data = df[[kpi_name, OUTCOME_COL]].dropna()
        n_obs = len(valid_data)

        logger.info(f"   Observaciones v√°lidas: {n_obs}")

        # Validar cantidad m√≠nima
        if n_obs < MIN_OBSERVATIONS_PER_KPI:
            logger.warning(f"   ‚ö†Ô∏è  INSUFICIENTE: Se necesitan al menos {MIN_OBSERVATIONS_PER_KPI}")
            results.append({
                'kpi_name': kpi_name,
                'pearson_corr': None,
                'pearson_pvalue': None,
                'spearman_corr': None,
                'spearman_pvalue': None,
                'n_observations': n_obs,
                'sufficient_data': False,
                'warning': f'Insuficientes datos (min: {MIN_OBSERVATIONS_PER_KPI})'
            })
            continue

        # Extraer arrays
        x = valid_data[kpi_name].values
        y = valid_data[OUTCOME_COL].values

        # Validar varianza
        if x.std() == 0 or y.std() == 0:
            logger.warning(f"   ‚ö†Ô∏è  Varianza cero")
            results.append({
                'kpi_name': kpi_name,
                'pearson_corr': None,
                'pearson_pvalue': None,
                'spearman_corr': None,
                'spearman_pvalue': None,
                'n_observations': n_obs,
                'sufficient_data': True,
                'warning': 'Varianza cero (valores constantes)'
            })
            continue

        # Calcular Pearson
        try:
            pearson_corr, pearson_pval = stats.pearsonr(x, y)
        except Exception as e:
            logger.error(f"   ‚ùå Error calculando Pearson: {e}")
            pearson_corr, pearson_pval = None, None

        # Calcular Spearman
        try:
            spearman_corr, spearman_pval = stats.spearmanr(x, y)
        except Exception as e:
            logger.error(f"   ‚ùå Error calculando Spearman: {e}")
            spearman_corr, spearman_pval = None, None

        # Log resultados
        if pearson_corr is not None:
            logger.info(f"   ‚úÖ Pearson:  r = {pearson_corr:+.4f}  (p = {pearson_pval:.4f})")
        if spearman_corr is not None:
            logger.info(f"   ‚úÖ Spearman: œÅ = {spearman_corr:+.4f}  (p = {spearman_pval:.4f})")

        # Interpretaci√≥n
        if pearson_corr is not None:
            strength = (
                "muy fuerte" if abs(pearson_corr) >= 0.7 else
                "fuerte" if abs(pearson_corr) >= 0.5 else
                "moderada" if abs(pearson_corr) >= 0.3 else
                "d√©bil"
            )
            direction = "positiva" if pearson_corr > 0 else "negativa"
            logger.info(f"   üìä Correlaci√≥n {strength} {direction}")

        # Guardar resultado
        results.append({
            'kpi_name': kpi_name,
            'pearson_corr': float(pearson_corr) if pearson_corr is not None else None,
            'pearson_pvalue': float(pearson_pval) if pearson_pval is not None else None,
            'spearman_corr': float(spearman_corr) if spearman_corr is not None else None,
            'spearman_pvalue': float(spearman_pval) if spearman_pval is not None else None,
            'n_observations': int(n_obs),
            'sufficient_data': True,
            'warning': None
        })

    # Resumen
    logger.info("\n" + "=" * 80)
    logger.info("RESUMEN DE CORRELACIONES")
    logger.info("=" * 80)

    valid_results = [r for r in results if r['sufficient_data'] and r['pearson_corr'] is not None]
    insufficient_results = [r for r in results if not r['sufficient_data']]

    logger.info(f"\n‚úÖ KPIs con datos suficientes: {len(valid_results)}")
    logger.info(f"‚ö†Ô∏è  KPIs con datos insuficientes: {len(insufficient_results)}")

    if valid_results:
        # Top 5 positivas
        top_positive = sorted(valid_results, key=lambda x: x['pearson_corr'], reverse=True)[:5]
        logger.info("\nüîù TOP 5 CORRELACIONES POSITIVAS:")
        for i, r in enumerate(top_positive, 1):
            logger.info(f"   {i}. {r['kpi_name']}: r = {r['pearson_corr']:+.4f} (n = {r['n_observations']})")

        # Top 5 negativas
        top_negative = sorted(valid_results, key=lambda x: x['pearson_corr'])[:5]
        if top_negative[0]['pearson_corr'] < 0:
            logger.info("\nüîª TOP 5 CORRELACIONES NEGATIVAS:")
            for i, r in enumerate(top_negative, 1):
                if r['pearson_corr'] < 0:
                    logger.info(f"   {i}. {r['kpi_name']}: r = {r['pearson_corr']:+.4f} (n = {r['n_observations']})")

    return results


# ============================================================================
# EXPORTACI√ìN
# ============================================================================

def export_results(results: List[Dict]) -> None:
    """
    Exporta resultados a CSV y JSON.

    Args:
        results: Lista de resultados de correlaciones
    """
    logger.info("\n" + "=" * 80)
    logger.info("EXPORTANDO RESULTADOS")
    logger.info("=" * 80)

    if not results:
        logger.warning("‚ö†Ô∏è  No hay resultados para exportar")
        return

    # Crear directorio de salida
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    csv_path = os.path.join(OUTPUT_DIR, 'kpi_correlations_mvp.csv')
    json_path = os.path.join(OUTPUT_DIR, 'kpi_correlations_mvp.json')

    # Exportar CSV
    try:
        df_results = pd.DataFrame(results)
        df_results.to_csv(csv_path, index=False)
        logger.info(f"‚úÖ CSV guardado en: {csv_path}")
    except Exception as e:
        logger.error(f"‚ùå Error al guardar CSV: {e}")

    # Exportar JSON
    try:
        output_data = {
            'metadata': {
                'analysis_date': datetime.now().isoformat(),
                'total_kpis': len(results),
                'kpis_with_sufficient_data': len([r for r in results if r['sufficient_data']]),
                'min_observations_threshold': MIN_OBSERVATIONS_PER_KPI,
                'tables_used': [TABLE_KPIS, TABLE_OUTCOMES],
                'outcome_column': OUTCOME_COL
            },
            'results': results
        }

        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, indent=2, ensure_ascii=False)

        logger.info(f"‚úÖ JSON guardado en: {json_path}")
    except Exception as e:
        logger.error(f"‚ùå Error al guardar JSON: {e}")


# ============================================================================
# FUNCI√ìN PRINCIPAL
# ============================================================================

def main():
    """
    Pipeline completo de an√°lisis de correlaciones (MVP simplificado).
    """
    logger.info("\n" + "=" * 80)
    logger.info("HRKEY - PROOF OF CORRELATION MVP (SIMPLIFICADO)")
    logger.info("=" * 80)
    logger.info(f"Fecha: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info(f"Tablas: {TABLE_KPIS}, {TABLE_OUTCOMES}")
    logger.info(f"Output: {OUTPUT_DIR}")

    try:
        # Paso 1: Construir dataset
        df = build_dataset()

        if df.empty:
            logger.error("\n‚ùå AN√ÅLISIS CANCELADO: No hay datos v√°lidos")
            logger.error("   Verifica que:")
            logger.error(f"   1. La tabla {TABLE_KPIS} tenga datos")
            logger.error(f"   2. La tabla {TABLE_OUTCOMES} tenga datos")
            logger.error("   3. Ambas tablas tengan la columna 'user_id'")
            logger.error(f"   4. La tabla {TABLE_OUTCOMES} tenga la columna '{OUTCOME_COL}'")
            return 1

        # Paso 2: Calcular correlaciones
        results = compute_correlations(df)

        if not results:
            logger.error("\n‚ùå AN√ÅLISIS CANCELADO: No se pudieron calcular correlaciones")
            return 1

        # Paso 3: Exportar resultados
        export_results(results)

        # Resumen final
        logger.info("\n" + "=" * 80)
        logger.info("‚úÖ AN√ÅLISIS COMPLETADO EXITOSAMENTE")
        logger.info("=" * 80)
        logger.info(f"\nüìä RESULTADOS:")
        logger.info(f"   Total KPIs analizados: {len(results)}")
        logger.info(f"   KPIs con datos suficientes: {len([r for r in results if r['sufficient_data']])}")
        logger.info(f"\nüìÅ ARCHIVOS GENERADOS:")
        logger.info(f"   CSV: ml/output/kpi_correlations_mvp.csv")
        logger.info(f"   JSON: ml/output/kpi_correlations_mvp.json")

        return 0

    except Exception as e:
        logger.error(f"\n‚ùå ERROR CR√çTICO: {e}", exc_info=True)
        return 1


# ============================================================================
# ENTRY POINT
# ============================================================================

if __name__ == "__main__":
    sys.exit(main())
