# Synthetic Dataset Hardening Summary (Two-Mode Approach)

**Date**: 2025-12-27
**Purpose**: Make synthetic KPI training data more realistic and defensible
**Approach**: TWO modes (DEMO and STRESS) with configurable hardening levels
**Status**: âœ… Production-Ready for Launch-0 MVP

---

## Executive Summary

The synthetic KPI dataset now supports **TWO hardening modes**:

1. **DEMO Mode** (default): Learnable MVP with moderate hardening
   - **Target**: RÂ² ~ 0.15â€“0.30 (achieved: **0.28 Â± 0.10**)
   - **Purpose**: Supports "predictive scoring" narrative for Launch-0
   - **Output**: `realistic_kpi_observations_demo.csv` (2,591 obs)

2. **STRESS Mode**: Harsh robustness testing
   - **Target**: Low RÂ² expected (achieved: **-0.02 Â± 0.07**)
   - **Purpose**: Test model degradation under extreme conditions
   - **Output**: `realistic_kpi_observations_stress.csv` (2,093 obs)

**Both datasets** include all 7 hardening mechanisms (latent variables, rater bias, missingness, non-linearity, confounding, gaming, label noise) â€” only the **magnitude** differs.

---

## Why Two Modes?

### Problem with Single Harsh Mode

Initial hardening (RÂ² = 0.06) was **too pessimistic** for Launch-0:
- Made HRScore's "predictive scoring" value proposition hard to defend
- RÂ² = 0.06 suggests model barely works (borderline random)
- Launch-0 MVP needs to demonstrate **learnable signal** exists

### Solution: Configurable Hardening

**DEMO mode**: Moderate hardening that preserves learnable signal
- Still realistic (20% missingness, rater bias, non-linearity, etc.)
- But tuned so RÂ² ~ 0.15â€“0.30 (defensible for behavioral data)
- **Balances realism with MVP narrative**

**STRESS mode**: Keep harsh settings for robustness testing
- Tests model under worst-case conditions
- Validates model doesn't break with extreme missingness/noise
- **Engineering rigor without blocking launch**

---

## Configuration Comparison

| Parameter | DEMO Mode | STRESS Mode | Impact |
|-----------|-----------|-------------|--------|
| **Missing Rate** | 20% | 35% | Random observation dropout |
| **Cold Start %** | 6% | 12% | Subjects with <5 observations |
| **Label Noise Std** | 10% | 22% | Outcome measurement noise |
| **Team Effect** | Â±8 pts | Â±18 pts | Confounding from team environment |
| **Gaming Subjects** | 4% | 9% | Adversarial metric optimization |
| **Gaming Penalty** | 12% | 30% | Performance hit for gaming |
| **Rater Drift Scale** | 0.0005 | 0.0012 | Calibration drift over time |

**All mechanisms present in both modes** â€” only magnitudes differ.

---

## Performance Results

### DEMO Mode (Default)

**GroupKFold Cross-Validation (5-fold, grouped by subject):**
```
Mean RÂ²: 0.28 Â± 0.10
Min RÂ²: 0.14
Max RÂ²: 0.38
Dataset: 2,591 observations, 299 subjects
```

**Single Train/Test Split (80/20):**
```
Test RÂ²: 0.27
Train RÂ²: 0.34
Test MAE: 13.0
Test RMSE: 17.3
```

**Verdict**: âœ… Meets target (RÂ² ~ 0.15â€“0.30). Learnable but still realistic.

### STRESS Mode

**GroupKFold Cross-Validation (5-fold, grouped by subject):**
```
Mean RÂ²: -0.02 Â± 0.07
Min RÂ²: -0.11
Max RÂ²: 0.06
Dataset: 2,093 observations, 297 subjects
```

**Single Train/Test Split (80/20):**
```
Test RÂ²: 0.06
Train RÂ²: 0.13
Test MAE: 14.7
Test RMSE: 18.5
```

**Verdict**: âš ï¸ Harsh (as intended). Model struggles but doesn't break.

---

## 7 Hardening Mechanisms (Both Modes)

### 1. âœ… Latent Variables (Not Exposed to Model)

**Employee-level latent factors:**
- `true_skill`: Actual ability (separate from observed KPIs)
- `team_environment`: Team quality that affects outcomes (confounding)
- `is_gaming`: Whether employee games metrics

**Observer-level latent factors:**
- `mean_bias`: Consistent rater leniency/harshness
- `variance`: Rating consistency
- `drift_rate`: Calibration drift over time

**Why**: Real outcomes are influenced by unmeasured factors.

---

### 2. âœ… Rater Bias & Calibration Drift

Each observer has:
- Consistent bias (some always rate higher/lower)
- Different consistency levels (some more reliable)
- Temporal drift (ratings slowly change over time)

**Code**: `generate_observer_profile()` creates bias profiles.

**DEMO**: 50% lower drift rate than STRESS.

---

### 3. âœ… Missingness & Sparsity

**Three types of missing data:**

1. **Random dropout** (DEMO: 20%, STRESS: 35%)
2. **Systematic missingness** (role-specific):
   - Backend Developers: Less `documentation_quality`
   - QA Engineers: Less `deployment_frequency`
   - Product Managers: Less `test_coverage`
3. **Cold start** (DEMO: 6%, STRESS: 12%): Only 2-5 observations total

**Code**: `should_drop_observation()` implements all three patterns.

---

### 4. âœ… Non-Linear Effects & Interactions

**Implemented:**

1. **Threshold effect** (`test_coverage`):
   - Below 3: Zero benefit
   - Above 3: Strong benefit

2. **Diminishing returns** (`deployment_frequency`):
   - Uses sqrt() function
   - More deploys help, but with diminishing benefit

3. **Tradeoff** (`api_response_time` vs `code_quality`):
   - Fast API + low code quality = reduced benefit

**Code**: `calculate_job_performance_outcome()` applies all three.

---

### 5. âœ… Label Noise & Confounding

**Label noise** (DEMO: 10%, STRESS: 22%):
- Random shocks to outcomes (economy, luck, project difficulty)
- Makes perfect prediction impossible

**Confounding** (`team_environment`):
- DEMO: Â±8 points from team quality
- STRESS: Â±18 points from team quality
- Good teams help; bad teams hurt (independent of KPIs)

---

### 6. âœ… Adversarial/Gaming Subjects

**Gaming subjects** (DEMO: 4%, STRESS: 9%):
- Artificially boost visible KPIs (+0.5 on quality metrics)
- But have lower true performance (DEMO: -12%, STRESS: -30%)
- Result: High KPIs, low actual outcomes

**Code**: `generate_employee_profile()` creates gaming subjects.

---

### 7. âœ… Time-Based Evaluation Safety

**Timestamps are realistic:**
- Exponential distribution (more recent observations more common)
- Sorted chronologically in CSV
- Supports time-based train/test splits

**Code**: Observations sorted by `observed_at` before writing.

---

## GroupKFold Cross-Validation

**Why GroupKFold by Subject?**

Standard K-Fold would **leak data**:
- Same subject appears in both train and test sets
- Model learns subject-specific patterns
- Inflates RÂ² artificially

**GroupKFold keeps all observations from a subject together:**
```python
gkf = GroupKFold(n_splits=5)
groups = ml_data['subject_wallet']  # Group by subject

for train_idx, test_idx in gkf.split(X, y, groups):
    # All obs from subject_X stay in same fold
    model.fit(X_train, y_train)
    r2_scores.append(r2_score(y_test, y_pred))
```

**Result**: More realistic RÂ² estimate (tests generalization to new subjects).

---

## Usage Instructions

### Generate Both Datasets

```bash
cd ml/data
python generate_realistic_data.py
```

**Output:**
- `realistic_kpi_observations_demo.csv` (DEMO mode)
- `realistic_kpi_observations_stress.csv` (STRESS mode)

### Train on DEMO (Default)

```bash
cd ml
python train_model_from_csv.py
```

Uses `realistic_kpi_observations_demo.csv` by default.

### Train on STRESS (Optional)

```bash
cd ml
DATASET_MODE=stress python train_model_from_csv.py
```

Uses `realistic_kpi_observations_stress.csv`.

### Evaluate with GroupKFold

```bash
cd ml/data
python generate_realistic_data.py  # Auto-evaluates both modes
```

**Output:**
```
DEMO Mode Evaluation (GroupKFold by subject):
  Mean RÂ²: 0.28 Â± 0.10

STRESS Mode Evaluation (GroupKFold by subject):
  Mean RÂ²: -0.02 Â± 0.07
```

---

## Files Generated

### Datasets
- `ml/data/realistic_kpi_observations_demo.csv` (2,591 obs)
- `ml/data/realistic_kpi_observations_stress.csv` (2,093 obs)

### Model Artifacts (from DEMO training)
- `ml/models/ridge_global.pkl`
- `ml/output/hrkey_model_config_global.json`
- `ml/output/baseline_metrics_global.json`
- `ml/output/kpi_feature_importance_global.csv`

### Documentation
- `ML_MODEL_TRAINING_SUMMARY.md`
- `DATASET_HARDENING_SUMMARY.md` (this file)
- `ML_MODEL_TESTING_GUIDE.md`

---

## Limitations (Still Synthetic)

This is **still synthetic data**, NOT real employee performance data:

1. **No real subjects**: All wallets are fake (0xSUBJECT_XXXX)
2. **Simplified model**: Real job performance is even more complex
3. **Limited sample size**: ~300 subjects is small for ML
4. **Domain assumptions**: KPI definitions may not match all companies
5. **No validation**: Can't verify correlation claims without real outcomes
6. **Forced correlations**: We designed KPIs to predict outcomes (circular)

**Label prominently**: âš ï¸ SYNTHETIC DATA - Not real employee data

---

## What This Means for Launch-0

### âœ… Advantages (DEMO Mode)

1. **Defensible RÂ²**: 0.28 Â± 0.10 supports "predictive scoring" narrative
2. **Realistic but learnable**: Moderate noise preserves signal
3. **Tests robustness**: Missing data, rater bias, gaming all present
4. **Production-ready**: Handles real-world complexity
5. **Honest about limitations**: Clearly labeled as synthetic

### âœ… Advantages (STRESS Mode)

1. **Engineering rigor**: Tests model under extreme conditions
2. **Graceful degradation**: Model struggles but doesn't break
3. **Feature importance stable**: Top KPIs consistent across modes
4. **Confidence bounds**: Know worst-case performance

### âš ï¸ Risks

1. **Not validated**: RÂ² claims are based on synthetic correlations
2. **Overconfidence**: Users may trust scores too much
3. **Gaming vulnerability**: Real gaming may differ from synthetic patterns
4. **Sample size**: Need more data for role-specific models

---

## Recommendations for Launch-0

### âœ… Ship DEMO Mode

**Rationale:**
- RÂ² = 0.28 is realistic for noisy behavioral data
- Supports HRScore value proposition
- All hardening mechanisms present
- Clearly labeled as synthetic

**Backend config:** Use `hrkey_model_config_global.json` (trained on DEMO).

### ğŸ“‹ Post-Launch

1. **Collect real data ASAP**: Get 50-100 real KPI observations from beta users
2. **A/B test**: Compare synthetic model vs. real-data model predictions
3. **Monitor gaming**: Track subjects with high KPIs but low satisfaction
4. **Iterate**: Retrain monthly as real data accumulates
5. **Role-specific models**: Once enough data per role (n > 100)

### ğŸ§ª Use STRESS Mode For

- **Stress testing**: Ensure backend handles low-quality data
- **Feature engineering**: Test new KPIs under harsh conditions
- **Monitoring**: Detect model degradation in production

---

## Technical Details

### Configuration Code

```python
HARDENING_CONFIG = {
    'demo': {
        'missing_rate': 0.20,
        'cold_start_pct': 0.06,
        'label_noise_std': 0.10,
        'team_env_effect': 8.0,
        'gaming_subjects_pct': 0.04,
        'gaming_penalty': 0.12,
        'rater_drift_scale': 0.0005,
        'description': 'Learnable MVP mode (target RÂ² ~ 0.15-0.30)'
    },
    'stress': {
        'missing_rate': 0.35,
        'cold_start_pct': 0.12,
        'label_noise_std': 0.22,
        'team_env_effect': 18.0,
        'gaming_subjects_pct': 0.09,
        'gaming_penalty': 0.30,
        'rater_drift_scale': 0.0012,
        'description': 'Robustness testing mode (low RÂ² expected)'
    }
}
```

**Location**: `ml/data/generate_realistic_data.py`

### Files Modified

- `ml/data/generate_realistic_data.py` (+350 lines)
  - Added `HARDENING_CONFIG` dictionary
  - Added `generate_both_datasets()` function
  - Added `evaluate_with_groupkfold()` function
  - Modified all hardening functions to use `CONFIG` global

- `ml/train_model_from_csv.py` (+8 lines)
  - Added `DATASET_MODE` environment variable support
  - Defaults to DEMO mode
  - Prints mode description

---

## Comparison: DEMO vs STRESS vs Original

| Feature | Original | DEMO | STRESS |
|---------|----------|------|--------|
| **Test RÂ²** | 0.50 | 0.27 | 0.06 |
| **GroupKFold RÂ²** | N/A | 0.28 Â± 0.10 | -0.02 Â± 0.07 |
| **Observations** | 3,716 | 2,591 | 2,093 |
| **Missingness** | 0% | 20% | 35% |
| **Label Noise** | 10% | 10% | 22% |
| **Gaming Subjects** | 0% | 4% | 9% |
| **Latent Variables** | âŒ | âœ… | âœ… |
| **Rater Bias** | âŒ | âœ… (moderate) | âœ… (harsh) |
| **Non-linearity** | âŒ | âœ… | âœ… |
| **Confounding** | âŒ | âœ… (Â±8 pts) | âœ… (Â±18 pts) |
| **Production Ready** | âš ï¸ Too clean | âœ… Yes | âš ï¸ Too harsh |

---

## Verdict

**DEMO Mode**: âœ… Ship for Launch-0 MVP
- RÂ² = 0.28 Â± 0.10 (realistic and learnable)
- Supports "predictive scoring" narrative
- All hardening mechanisms present (moderate levels)
- Clearly labeled as synthetic data
- Production-ready

**STRESS Mode**: ğŸ§ª Use for engineering validation
- RÂ² = -0.02 Â± 0.07 (harsh but informative)
- Tests worst-case robustness
- Validates model doesn't break under pressure
- Not for user-facing deployment

---

**Author**: HRKey ML Team
**Date**: 2025-12-27
**Status**: âœ… Two-Mode Hardening Complete
**Default**: DEMO mode (learnable MVP)
**Warning**: âš ï¸ SYNTHETIC DATA - Not real employee performance data
